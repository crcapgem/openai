
# Generate Podcast Synopsis
%load_ext autoreload
%autoreload 2

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# Load Data
fname = "../data/interviews_transcript.txt"

with open(fname,  'r', errors='replace') as f:
  content = f.readlines()

# convert list to str
content =' '.join(content) 
#print(content)

# Set up Azure OpenAI
import os
import openai

# Set up Azure OpenAI
openai.api_type = "azure"
openai.api_base = 'https://azure-openai-testcgo.openai.azure.com/'
openai.api_version = "2023-03-15-preview"
openai.api_key = '8113c98b58054e3892f97386191e9e39'

# Deploy a Model
# id of desired_model
desired_model = 'gpt-35-turbo' # suitable for text generation
desired_capability = 'completion'

# list models deployed with
deployment_id = None
result = openai.Deployment.list()

for deployment in result.data:
    if deployment["status"] != "succeeded":
        continue
    
    model = openai.Model.retrieve(deployment["model"])

    # check if desired_model is deployed, and if it has 'completion' capability
    if model["id"] == desired_model and model['capabilities'][desired_capability]:
        deployment_id = deployment["id"]
        
# if no model deployed, deploy one
if not deployment_id:
    print('No deployment with status: succeeded found.')

    # Deploy the model
    print(f'Creating a new deployment with model: {desired_model}')
    result = openai.Deployment.create(model=desired_model, scale_settings={"scale_type":"standard"})
    deployment_id = result["id"]
    print(f'Successfully created {desired_model} that supports text {desired_capability} with id: {deployment_id}.')
else:
    print(f'Found a succeeded deployment of "{desired_model}" that supports text {desired_capability} with id: {deployment_id}.')
    

# Text chunk generator
# A generator that split a text into smaller chunks of size n, preferably ending at the end of a sentence
def chunk_generator(text, n, tokenizer):
    tokens = tokenizer.encode(text)
    i = 0
    while i < len(tokens):
        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens
        j = min(i + int(1.5 * n), len(tokens))
        while j > i + int(0.5 * n):
            # Decode the tokens and check for full stop or newline
            chunk = tokenizer.decode(tokens[i:j])
            if chunk.endswith(".") or chunk.endswith("\n"):
                break
            j -= 1
        # If no end of sentence found, use n tokens as the chunk size
        if j == i + int(0.5 * n):
            j = min(i + n, len(tokens))
        yield tokens[i:j]
        i = j

# Request API

def request_api(document, prompt_postfix, max_tokens):
    prompt = prompt_postfix.replace('<document>',document)
    #print(f'>>> prompt : {prompt}')

    response = openai.Completion.create(  
    deployment_id=deployment_id, 
    prompt=prompt,
    temperature=0,
    max_tokens=max_tokens,
    top_p=1,
    frequency_penalty=1,
    presence_penalty=1,
    stop='###')

    return response['choices'][0]['text']

# Generate Synopsis

def get_synopsis(content, prompt_postfix):
    import tiktoken

    synopsis_chunck = []
    n = 2000 # max tokens for chuncking
    max_tokens = 1000 # max tokens for response

    tokenizer = tiktoken.get_encoding('p50k_base')

    # Generate chunkcs    
    chunks = chunk_generator(content, n, tokenizer)

    # Decode chunk of text
    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]

    # Request api
    for chunk in text_chunks:
        synopsis_chunck.append(request_api(chunk, prompt_postfix, max_tokens))
        #print(chunk)
        #print('>>> synopsis: \n' + synopsis_chunck[-1])

    # Synopsis
    synopsis = ' '.join(synopsis_chunck)

    return synopsis
    
# Prompt postfix
prompt_postfix = """ <document>
  \n###
  \nSummarise the transcript of a podcast above into a synopsis. 
  \nSynopsis : 
"""
#print(prompt_postfix)

synopsis = get_synopsis(content, prompt_postfix)

print(synopsis)

"""
Silicon Valley Bank (SVB) collapsed last week, the biggest bank failure since 2008. The collapse was caused by bad decisions at the bank and a rapid increase in interest rates. SVB's balance sheet had two weird features: on the deposit side they were almost entirely funded by business depositors who demand more interest when rates go up; on the lending side, all of these small companies that are Silicon Valley Bankâ€™s core customers got huge amounts of money in and deposited it at Silicon Valley Bank. So their deposits quadrupled in a couple of years, or at least tripled in a couple of years. They had so much money they couldn't even loan it out as fast as it was coming in so bought Treasury bonds which have fixed interest rates - this meant that while costs for getting money went up with rising interest rates, profits from giving out loans did not rise because those assets' prices remained fixed.
 
The government has stepped into to ensure SVB's customers get their money back but there is concern about whether our banking system is secure enough. However, Robert Armstrong argues that everything should be fine if people don't panic because most individual banks are solvent and governments will take action to protect depositors if necessary.

Armstrong says what happened with SVB isn't like 2008 where there was a credit event due to undercapitalised bad credit on housing loans melting down; instead he sees this as an outlier case of poor balance sheet management hopefully ending with just one or two other banks making similar mistakes.

He also points out that US banking operates under a two-tier regulatory system where very big banks have stricter rules than smaller ones which might find meeting all those requirements prohibitively expensive given their scale disadvantage compared to bigger players.
<|im_end|> The collapse of a small US bank, First NBC Bank Holding Company, has raised concerns about the health of other banks in America. The bank's failure was caused by its rapid expansion and overexposure to commercial real estate loans. However, it is unlikely that this will lead to another financial crisis as the banking system is better capitalised than before 2008 and regulators have learned important lessons from that time. Nevertheless, there are questions around whether having a two-tier regulatory structure for large and smaller banks is effective or if more needs to be done to prevent risk building up on balance sheets.
  
"""

# Importing Required Libraries
import nltk
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize,sent_tokenize 

# Tokenizing Sentences
sentences = sent_tokenize(text)

# Removing Stopwords & Punctuations And Creating Frequency Table
stop_words = set(stopwords.words('english')) 
word_frequencies = {}  
for sentence in sentences:  
    tokens = word_tokenize(sentence)  
    for token in tokens:  
...
Output : 

The collapse of a small US bank, First NBC Bank Holding Company, has raised concerns about the health of other banks in America. It could be that regulators didn’t think hard enough about the threat posed by the large amount of securities building up on bank balance sheets. They might have to think harder about whether that’s a risk they need to do something about. There’s definitely people who disagree with me but maturity transformation will always be fundamentally magical and there will always be scary moments.The goal of regulation is not to bring the risk for everyone down to zero.I described this business of taking short-term money in and putting long-term money out as black magic.What I would say is this: If I am going to put my company’s operating budget and its investment capital into a bank,I’d probably want thatto bediversifiedbank.AndI knowthat’ s alotto askofbusinesspeople.Butdoesbankhaveplentyofcapital?Doesithaveavarietyofdepositclients?Doeshavevarietyofdifferentkindsofloansontheassetsideofitsbalancesheet?
"""<|im_end|>
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
"""

# Translate Synopsis

# Prompt postfix
prompt_postfix = """ <document>
  \n###
  \nTranslate synopsis into Spanish.  
  \nTranslation : 
"""
print(prompt_postfix)

max_tokens = 1000
translation = request_api(synopsis, prompt_postfix, max_tokens)
print(translation)

# Generate Taglines
# Prompt postfix
prompt_postfix = """ <document>
  \n###
  \nGenerate 2 to 3 tag lines based on the podcast synopsis above.
"""
#print(prompt_postfix)

max_tokens = 500
tag_lines = request_api(synopsis, prompt_postfix, max_tokens)
print(tag_lines)



# Generate Search Engine Optimised (SEO) keywords
# Prompt postfix
prompt_postfix = """ <document>
  \n###
  \nGenerate 5 search engine optimised keywords based on text above.  
"""
#print(prompt_postfix)

def get_keywords(content, prompt_postfix):
    import tiktoken

    keywords_chunck = []
    n = 2000 # max tokens for chuncking
    max_tokens = 100

    tokenizer = tiktoken.get_encoding('p50k_base')

    # Generate chunkcs    
    chunks = chunk_generator(content, n, tokenizer)

    # Decode chunk of text
    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]

    # Request api
    for chunk in text_chunks:
        keywords_chunck.append(request_api(chunk, prompt_postfix, max_tokens))

    # Keywords
    keywords = ' '.join(keywords_chunck)
    return keywords
    
keywords = get_keywords(synopsis, prompt_postfix)
print(keywords)

